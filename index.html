<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>
        Z-GMOT with MA-SORT: Zero-shot Generic Multiple Object Tracking (GMOT) with Motion Appearance SORT (MA-SORT)
    </title>
    <meta content="Z-GMOT" property="og:title" />
    <meta content="A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks." name="description" property="og:description" />
    <meta content="https://github.com/DanceTrack" property="og:url" />
    <meta name="keywords" content="Generic Multi-Object Tracking in Uniform Appearance and Diverse Motion">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script defer src="js/fontawesome.all.min.js"></script>
</head>

<body>
    <div class="navbar">
        <h3>Z-GMOT demo website</h3>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="result.html">Result</a></li>
            <li><a href="code.html">Code</a></li>
            <li><a href="dataset.html">Dataset</a></li>
        </ul>
        <script>
            // Get the current URL
            var currentURL = window.location.href;
        
            // Select all navigation links
            var navLinks = document.querySelectorAll('.navbar a');
        
            // Loop through the links to find the active one
            for (var i = 0; i < navLinks.length; i++) {
                var linkURL = navLinks[i].href;
        
                // Check if the current URL contains the link's URL
                if (currentURL.indexOf(linkURL) !== -1) {
                    // Add the "active" class to the link
                    navLinks[i].classList.add('active');
                }
            }
        </script>
    </div>
    

    <div class="n-title">
        <h1>
            Z-GMOT with MA-SORT: Zero-shot Generic Multiple Object Tracking (GMOT) with Motion Appearance SORT (MA-SORT)
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    This work has been submitted to NAACL 2024
                </li>
            </ul>
            <p style="text-align: center; color: slategray">PDF version will be published soon</p>
            <!-- <ul class="authors affiliations">
                <li>
                    Anonymous
                </li>
            </ul> -->
            <ul class="authors links">
                <!-- <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Code</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Dataset</button>
                    </a>
                </li>  -->
                <!-- <li>
                    <a href="https://www.youtube.com/playlist?list=PLSf1X3oNUW2se2U111HJ3wkSnZ7dcVGC_" target="_blank">
                        <button class="btn"><i class="fab fa-youtube fa-w-18"></i> Demo</button>
                    </a>
                </li> -->
            </ul>
        </div>
    </div>

<!-- <iframe width="760" height="381" src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    
    <div class="n-article">
        
        <div>
            <img class="figure" src="media/teaser.png" width="100%" alt="method comparison">
            <p class="figure-name"><b>Figure 1</b>: Comparison between existing methods with our current framework Z-GMOT for zero shot object detection</p>
        </div>

        <h2 id="abstract">
            Abstract
        </h2>
        <p>
            Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories, and struggles with unseen objects. To address this, Generic Multiple Object Tracking (GMOT) requires less prior information, but existing GMOT methods rely on initial bounding boxes and struggle with variations e.g., viewpoint, lighting, occlusion, scale, etc. We propose <b>Z-GMOT</b>, capable of tracking never-seen categories with no training examples, without predefined categories or initial bounding boxes. Our approach includes <b>iGLIP</b>, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. Addressing challenges in tracking high-similarity objects within the same category, we introduce <b>MA-SORT</b>, which integrates motion and appearance-based matching. Extensive evaluation on GMOT-40 dataset, AnimalTrack test set, DanceTrack validation set demonstrates substantial improvements. Our code and models will be publicly available upon paper acceptance.
        </p>

        <!------------------------------------------------------Section Boundary-------------------------------------------------->
        
        <h2 id="limitation">
            Limitations
        </h2>
        <p style="font-size: 22px;">
            <b>What are the problems with current MOT methods?</b>
        </p>
        <p>
            Multiple Object Tracking (MOT) plays a crucial role in diverse real-world scenarios, such as surveillance, security, autonomous driving, robotics, biology, etc. 
            However, current MOT methods suffer from several limitations: they heavily depend on prior knowledge of tracking targets, requiring costly annotation efforts for large labeled datasets; they struggle with tracking unseen categories and objects of specific categories; they are limited in handling objects with indistinguishable appearances. 
        </p>

        <p style="font-size: 22px;">
            <b>Vision Language model as detector - GLIP:</b>
        </p>
        <p>
            In recent years, the abundance of image-text pairs on the Internet has given rise to the emergence of Vision-Language (VL) models, which combine the ability to comprehend visual data with natural language processing capabilities. Apart from traditional one-hot vector labels, semantic meanings and captions can be utilized, making CV tasks more flexible and capable of handling open-world scenarios by identifying novel concepts in real-world applications and recognizing previously unseen categories.
            <br>However, we empirically noticed several limitations of GLIP as follows
        </p>
        <p>
            <b><u><span style="font-size: 20px;">Limitation 1 - High False Positives:</span></u></b> 
            Current object recognition models, like GLIP, perform well for general categories (e.g., 'car') but struggle with 
            specific details (e.g., 'red car'). At high thresholds, important objects are often missed, while at low thresholds, 
            incorrect objects are identified. Balancing this trade-off is challenging, making it hard to achieve optimal performance 
            for specific object categories.
        </p>
        <div>
            <img class="figure" src="media/limit1.png" width="100%" alt="Limitation 1">
        </div>

        <p>
            <b><u><span style="font-size: 20px;">Limitation 2 - Deep fusion in GLIP weakens proposal features when dealing with specific object category:</span></u></b> 
            GLIP uses text embedding to process and represent information, but it faces challenges in recognizing specific object 
            categories. The impact of text embedding varies, either enhancing (for general objects) or weakening (for specific objects) 
            the features of proposed objects. This inconsistency affects the overall performance of GLIP, especially for categories 
            that involve detailed property descriptions.
        </p>
        <div>
            <img class="figure" src="media/limit2.png" width="100%" alt="Limitation 2">
        </div>

        <!------------------------------------------------------Section Boundary-------------------------------------------------->
        
        <h2 id="zgmot">
            Z-GMOT
        </h2>
        <div>
            <img class="figure" src="media/ZGMOT.jpg" width="100%" alt="Pipeline Overview">
        </div>
        <p>
            Based on the advancements of recent works in multi-modal, we propose a method which we believe is the future of multi-object tracking - Z-GMOT, a cutting-edge paradigm that seamlessly integrates Natural Language Processing (NLP) to redefine the way we perceive and interact with visual data. Z-GMOT stands at the forefront of innovation, offering unparalleled precision in tracking objects by leveraging advanced linguistic input.
        </p>
        <p>
            Above we have an example of a single image encapsulates a multitude of intricacies: a bustling scene with various cars, each unique in its color and characteristics. With Z-GMOT, the paradigm transcends traditional limitations, effortlessly tracking not only the generic "all cars" but also delving into the realm of specificity with distinctions such as "red car," "yellow car," and "blue car."
        </p>
        <p>
            In our exclusive demonstration, you can see the proficiency of Z-GMOT as it navigates through a captivating image, dynamically tracking and distinguishing between cars based on their color. Picture the finesse with which this paradigm adapts to your natural language queries, discerning the subtle nuances that set each car apart from the rest.
        </p>


        <!-- <h2 id="rebuttal-aaai">
            Rebuttal Section
        </h2>
        <div class="chat-container">
            <div class="recipient-message">
                <strong>Reviewer #1:</strong> Hello, how are you today?
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> I'm doing well, thank you! How can I help you?
            </div>
            <br>
    
            <div class="recipient-message">
                <strong>Reviewer #2:</strong> I have a question about your product.
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> Sure, I'd be happy to help. What's your question?
            </div>
            <br>
        </div>
        <p> </p> -->
        

        <h2 id="method">
            Methodology
        </h2>
        <p>
            In our approach, we've developed innovative techniques to improve the way we track objects in diverse scenarios. We introduce two key components: iGLIP and MA-SORT.
        </p>
        
        <h3>iGLIP: Smarter Object Recognition</h3>
        <p>
            To overcome challenges in recognizing specific objects, we've enhanced GLIP. Using improved prompts and a clever matching mechanism, iGLIP accurately identifies objects, even those with detailed characteristics, without being misled by irrelevant information.
        </p>
        <img class="figure" src="media/glip-iglip.png" width="100%" alt="iGLIP Overview">
        
        <h3>MA-SORT: More Accurate Tracking</h3>
        <p>
            Our second component, MA-SORT, boosts tracking accuracy by combining motion and appearance cues. This means our system can follow objects more precisely, especially in situations where appearance alone might be ambiguous. By integrating these two factors, MA-SORT adapts dynamically to different scenarios, making object tracking more reliable.
        </p>
    </div>

    <footer>
        <div class="footer-content">
            <p style="text-align: center;">&copy; Website for NAACL24's submission</p>
        </div>
    </footer>
</body>

</html>
    
                   
