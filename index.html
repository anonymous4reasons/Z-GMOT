<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>
        Z-GMOT with MA-SORT: Zero-shot Generic Multiple Object Tracking (GMOT) with Motion Appearance SORT (MA-SORT)
    </title>
    <meta content="Open-GMOT" property="og:title" />
    <meta content="A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks." name="description" property="og:description" />
    <meta content="https://github.com/DanceTrack" property="og:url" />
    <meta name="keywords" content="Generic Multi-Object Tracking in Uniform Appearance and Diverse Motion">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script defer src="js/fontawesome.all.min.js"></script>
</head>

<body>
    <div class="navbar">
        <h3>Z-GMOT demo website</h3>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="result.html">Result</a></li>
            <li><a href="code.html">Code</a></li>
            <li><a href="dataset.html">Dataset</a></li>
        </ul>
        <script>
            // Get the current URL
            var currentURL = window.location.href;
        
            // Select all navigation links
            var navLinks = document.querySelectorAll('.navbar a');
        
            // Loop through the links to find the active one
            for (var i = 0; i < navLinks.length; i++) {
                var linkURL = navLinks[i].href;
        
                // Check if the current URL contains the link's URL
                if (currentURL.indexOf(linkURL) !== -1) {
                    // Add the "active" class to the link
                    navLinks[i].classList.add('active');
                }
            }
        </script>
    </div>
    

    <div class="n-title">
        <h1>
            Z-GMOT with MA-SORT: Zero-shot Generic Multiple Object Tracking (GMOT) with Motion Appearance SORT (MA-SORT)
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    This work has been submitted to NAACL 2024
                </li>
            </ul>
            <p style="text-align: center; color: slategray">PDF version will be published soon</p>
            <!-- <ul class="authors affiliations">
                <li>
                    Anonymous
                </li>
            </ul> -->
            <ul class="authors links">
                <!-- <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Code</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Dataset</button>
                    </a>
                </li>  -->
                <!-- <li>
                    <a href="https://www.youtube.com/playlist?list=PLSf1X3oNUW2se2U111HJ3wkSnZ7dcVGC_" target="_blank">
                        <button class="btn"><i class="fab fa-youtube fa-w-18"></i> Demo</button>
                    </a>
                </li> -->
            </ul>
        </div>
    </div>

<!-- <iframe width="760" height="381" src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    
    <div class="n-article">

        <h2 id="introduction">
            Introduction
        </h2>
        <p>
            Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories, and struggles with unseen objects. To address this, Generic Multiple Object Tracking (GMOT) requires less prior information, but existing GMOT methods rely on initial bounding boxes and struggle with variations e.g., viewpoint, lighting, occlusion, scale, etc. We propose <b>Z-GMOT</b>, capable of tracking never-seen categories with no training examples, without predefined categories or initial bounding boxes. Our approach includes <b>iGLIP</b>, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. Addressing challenges in tracking high-similarity objects within the same category, we introduce <b>MA-SORT</b>, which integrates motion and appearance-based matching. Extensive evaluation on GMOT-40 dataset, AnimalTrack test set, DanceTrack validation set demonstrates substantial improvements. Our code and models will be publicly available upon paper acceptance.
        </p>
        <div>
            <img class="figure" src="media/Fig1.png" width="100%" alt="Pipeline Overview">
        </div>


        <!-- <h2 id="rebuttal-aaai">
            Rebuttal Section
        </h2>
        <div class="chat-container">
            <div class="recipient-message">
                <strong>Reviewer #1:</strong> Hello, how are you today?
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> I'm doing well, thank you! How can I help you?
            </div>
            <br>
    
            <div class="recipient-message">
                <strong>Reviewer #2:</strong> I have a question about your product.
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> Sure, I'd be happy to help. What's your question?
            </div>
            <br>
        </div>
        <p> </p> -->
        
        <!------------------------------------------------------Section Boundary-------------------------------------------------->
        <h2 id="method">
            Methodology
        </h2>
        <p style="font-size: 22px;">
            <b>Limitations</b>: We empirically observe several limitations of GLIP as follows:
        </p>
        <p>
            <b><u><span style="font-size: 20px;">Limitation 1 - High False Positives:</span></u></b> GLIP demonstrates reliable performance for the general object category (<sup>OC<sup>Gen</sup></sup>); 
            however, it struggles to handle specific object categories (<sup>OC<sup>Spe</sup></sup>) with property descriptions, such as color 
            or shape. As shown in Figure 1, when various thresholds <i>t</i> are applied, the outcomes of GLIP's detection model for 
            <sup>OC<sup>Gen</sup></sup> prompt (e.g., <em>'car'</em>) remain relatively stable. In contrast, for the <sup>OC<sup>Spe</sup></sup> prompt (e.g., <em>'red car'</em>), 
            the detected bounding boxes exhibit high variability, with true positives (TP) missing at high thresholds and false positives (FP) present at low thresholds. The trade-off 
            between high TP and low FP makes it difficult to achieve optimal performance.
        </p>
        <div>
            <img class="figure" src="media/limit1.png" width="100%" alt="Limitation 1">
        </div>

        <p>
            <b><u><span style="font-size: 20px;">Limitation 2 - Deep fusion in GLIP weakens proposal features when dealing with specific object category:</span></u></b> 
            The role of text embedding is critical in GLIP, as it has a significant impact on both token and proposal features. 
            The token features <span>${P^i}$</span> and proposal features <span>${O^i}$</span> are fused through a deep fusion module, 
            and thus the text embedding continues to influence proposal features throughout this module. In Fig. 2, we visualize the impact of 
            the text embedding on GLIP's performance with the same threshold <i>t</i> for two cases of <sup>OC<sup>Gen</sup></sup> (i.e. 'car') 
            and <sup>OC<sup>Spe</sup></sup> (i.e., 'red car'). While we obtain similar results on <sup>OC<sup>Gen</sup></sup>, the performance 
            of GLIP is not reliable in the case of <sup>OC<sup>Spe</sup></sup>, even at the same threshold. Our empirical experiments suggest 
            that the text embedding may either strengthen (in the case of <sup>OC<sup>Gen</sup></sup>) or weaken (in the case of <sup>OC<sup>Spe</sup></sup>) proposal features, 
            despite the presence of strong visual feature
        </p>
        <div>
            <img class="figure" src="media/limti2.png" width="100%" alt="Limitation 2">
        </div>
        <p style="font-size: 22px;">
            <b>iGLIP</b>: In order to overcome the limitations of GLIP as discussed earlier, we propose iGLIP.
        </p>
        <p>To address <i>Limitation 1</i>, we propose a strategy where a <strong>high threshold 
            <span style="font-family: 'Times New Roman', Times, serif;">𝒯</span><sup>h</sup></strong> is used for 
            <sup>OC<sup>Spe</sup></sup> and a <strong>low threshold <span style="font-family: 'Times New Roman', Times, serif;">𝒯</span><sup>l</sup></strong> 
            for <sup>OC<sup>Gen</sup></sup>. This way, only TP (no FP) are detected in <sup>OC<sup>Spe</sup></sup>, while allowing for the 
            acceptance of FP in the <sup>OC<sup>Gen</sup></sup>. The proposals from <sup>OC<sup>Spe</sup></sup> are treated as queries, and 
            a <strong>Query-Guided Matching</strong> mechanism is proposed to filter all FP proposals from <sup>OC<sup>Gen</sup></sup>. 
            Moreover, to address <i>Limitation 2</i>, we propose to utilize the rich visual features obtained from the backbone 
            <span style="font-family: 'Times New Roman', Times, serif;">O<sup>0</sup></span> instead of the last layer feature 
            <span style="font-family: 'Times New Roman', Times, serif;">O<sup>L</sup></span> in our Query-Guided Matching. The overall 
            architecture of our proposed iGLIP is illustrated in Fig. 2 (B).</p>

        <p>iGLIP takes an input image <span style="font-family: 'Times New Roman', Times, serif;">I</span> and two kinds of prompts, namely, the specific prompt (<span style="font-family: 'Times New Roman', Times, serif;">𝑇<sub>s</sub></span>) and the general prompt (<span style="font-family: 'Times New Roman', Times, serif;">𝑇<sub>g</sub></span>). Both kinds of prompts go through a text encoder, i.e., BERTModule, to obtain contextual word features (i.e., <span style="font-family: 'Times New Roman', Times, serif;">P<sub>s</sub><sup>0</sup></span> and <span style="font-family: 'Times New Roman', Times, serif;">P<sub>g</sub><sup>0</sup></span>). Meanwhile, the input image goes through a visual encoder, i.e., DyHeadModule, to obtain proposal features (i.e., <span style="font-family: 'Times New Roman', Times, serif;">O<sup>0</sup></span>). Then, <span style="font-family: 'Times New Roman', Times, serif;">L</span> deep fusion layers are applied into <span style="font-family: 'Times New Roman', Times, serif;">P<sub>s</sub><sup>0</sup></span>, <span style="font-family: 'Times New Roman', Times, serif;">P<sub>g</sub><sup>0</sup></span>, and <span style="font-family: 'Times New Roman', Times, serif;">O<sup>0</sup></span>. Finally, the word-region alignment module is utilized to calculate the alignment score by performing a dot product between the deep fused features.</p>

        <p>where <span style="font-family: 'Times New Roman', Times, serif;">O<sub>s</sub></span> = <span style="font-family: 'Times New Roman', Times, serif;">O<sup>L</sup></span> 
            in <span style="font-family: 'Times New Roman', Times, serif;">ℝ<sup>N×d</sup></span>, <span style="font-family: 'Times New Roman', Times, serif;">O<sub>g</sub></span> = 
            <span style="font-family: 'Times New Roman', Times, serif;">O<sup>L</sup></span> in <span style="font-family: 'Times New Roman', Times, serif;">ℝ<sup>N×d</sup></span> 
            are the visual features from the last visual encoder layer and <span style="font-family: 'Times New Roman', Times, serif;">P<sub>s</sub></span> = 
            <span style="font-family: 'Times New Roman', Times, serif;">P<sup>L</sup><sub>s</sub></span> in <span style="font-family: 'Times New Roman', Times, serif;">ℝ<sup>M×d</sup></span>, 
            <span style="font-family: 'Times New Roman', Times, serif;">P<sub>g</sub></span> = <span style="font-family: 'Times New Roman', Times, serif;">P<sup>L</sup><sub>g</sub></span> in 
            <span style="font-family: 'Times New Roman', Times, serif;">ℝ<sup>M×d</sup></span> are the word features of <sup>OC<sup>Spe</sup></sup> and <sup>OC<sup>Gen</sup></sup> from the last language encoder layer. 
            The result of this operation are matrices <span style="font-family: 'Times New Roman', Times, serif;">S<sub>s</sub><sup>align</sup></span> in <span style="font-family: 'Times New Roman', Times, serif;">ℝ<sup>N×M</sup></span>, 
            <span style="font-family: 'Times New Roman', Times, serif;">S<sub>g</sub><sup>align</sup></span> in <span style="font-family: 'Times New Roman', Times, serif;">ℝ<sup>N×M</sup></span>. 
            Each element <span style="font-family: 'Times New Roman', Times, serif;">S<sub>s</sub><sup>align</sup>(i,j)</span> represents the alignment score between the 
            <span style="font-family: 'Times New Roman', Times, serif;">i</span>-th proposal region and <sup>OC<sup>Spe</sup></sup>. Each element 
            <span style="font-family: 'Times New Roman', Times, serif;">S<sub>g</sub><sup>align</sup>(i,j)</span> represents the alignment score between the 
            <span style="font-family: 'Times New Roman', Times, serif;">i</span>-th proposal region and <sup>OC<sup>Gen</sup></sup>.
        </p>

        <p>The resulting bounding boxes are filtered by different thresholds, where a <strong>high threshold <span style="font-family: 'Times New Roman', Times, serif;">𝒯</span><sup>h</sup></strong> 
            is applied into <span style="font-family: 'Times New Roman', Times, serif;">S<sub>s</sub><sup>align</sup></span> to ensure only TP are detected, and 
            a <strong>low threshold <span style="font-family: 'Times New Roman', Times, serif;">𝒯</span><sup>l</sup></strong> is applied into 
            <span style="font-family: 'Times New Roman', Times, serif;">S<sub>g</sub><sup>align</sup></span> to include all objects proposals, 
            which may include FP. All the TP obtained by <span style="font-family: 'Times New Roman', Times, serif;">S<sub>s</sub><sup>align</sup></span> 
            are treated as a queries set <span style="font-family: 'Times New Roman', Times, serif;">𝒬<sub>q</sub></span> (i.e., template patterns), 
            while all bounding boxes obtained by <span style="font-family: 'Times New Roman', Times, serif;">S<sub>g</sub><sup>align</sup></span> form the 
            target set <span style="font-family: 'Times New Roman', Times, serif;">𝒬<sub>t</sub></span>. <i>Query-Guided Matching</i> module is then proposed to 
            eliminate FP in <span style="font-family: 'Times New Roman', Times, serif;">𝒬<sub>t</sub></span> by using <span style="font-family: 'Times New Roman', Times, serif;">𝒬<sub>q</sub></span> 
            as template patterns.
        </p>

        <div>
            <img class="figure" src="media/glip-iglip.png" width="100%" alt="Limitation 1">
        </div>

        <p>
            <b><u><span style="font-size: 20px;">Query-Guided Matching:</span></u></b>
        </p>
        


        <p style="font-size: 22px;">
            <b>MA-SORT</b>: We introduce MA-SORT, as an enhancement of OC-SORT, which itself extends the SORT
        </p>
        <p>
            The standard similarity between track and box embeddings is defined using cosine distance, denoted as <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝐂<sub>𝑎</sub> ∈ ℜ<sup>𝑀×𝑁</sup></span>. In a typical tracking approach that combines visual appearance and motion cues, the cost matrix <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝐶</span> is computed as <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝐶(𝒯, 𝒟) = 𝑀<sub>c</sub>(𝒯, 𝒟) + 𝛼 𝐶<sub>𝑎</sub>(𝒯, 𝒟)</span>, where <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝑀<sub>c</sub></span> represents the motion cost, measured by the IoU cost matrix. Leveraging OC-SORT, which computes a virtual trajectory over the occlusion period to rectify the error accumulation of filter parameters during occlusions, the motion cost is defined as <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝑀<sub>c</sub>(𝒯, 𝒟) = IoU(𝒯, 𝒟) + 𝜆 𝐶<sub>𝑣</sub>(𝜏, 𝒯)</span>. Thus, the resulting cost matrix integrating both visual appearance and motion is as follows:
        </p>
          
        <figure>
          <p style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold; text-align: center;">
            𝐶(𝒯, 𝒟) = IoU(𝒯, 𝒟) + 𝜆 C<sub>𝑣</sub>(𝜏, 𝒯) + 𝛼 C<sub>𝑎</sub>(𝒯, 𝒟)
          </p>
          <figcaption style="text-align: center;">(Equation 1)</figcaption>
        </figure>
        <p>
          As the weight on appearance decreases, we propose redistributing the remaining weight to motion. Thus, the adaptive motion weight 
          <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝑊<sub>amw</sub></span> is defined as:
        </p>
        
        <figure>
          <p style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold; text-align: center;">
            𝑊<sub>amw</sub> = 1 + [1 - 𝑊<sub>aaw</sub>] = 2 - [1 - 𝜇<sub>det</sub> / (1 - cos(45°))]
          </p>
          <figcaption style="text-align: center;">(Equation 2)</figcaption>
        </figure>
    </div>

    <footer>
        <div class="footer-content">
            <p style="text-align: center;">&copy; Website for NAACL24's submission</p>
        </div>
    </footer>
</body>

</html>
    
                   
