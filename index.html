<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>
        Z-GMOT with MA-SORT: Zero-shot Generic Multiple Object Tracking (GMOT) with Motion Appearance SORT (MA-SORT)
    </title>
    <meta content="Z-GMOT" property="og:title" />
    <meta content="A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID) for object association. This pipeline is partially motivated by recent progress in both object detec- tion and re-ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin- guishing appearance and re-ID models are sufficient for es- tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have similar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks." name="description" property="og:description" />
    <meta content="https://github.com/DanceTrack" property="og:url" />
    <meta name="keywords" content="Generic Multi-Object Tracking in Uniform Appearance and Diverse Motion">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script defer src="js/fontawesome.all.min.js"></script>
</head>

<body>
    <div class="navbar">
        <h3>Z-GMOT demo website</h3>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="result.html">Result</a></li>
            <li><a href="code.html">Code</a></li>
            <li><a href="dataset.html">Dataset</a></li>
        </ul>
        <script>
            // Get the current URL
            var currentURL = window.location.href;
        
            // Select all navigation links
            var navLinks = document.querySelectorAll('.navbar a');
        
            // Loop through the links to find the active one
            for (var i = 0; i < navLinks.length; i++) {
                var linkURL = navLinks[i].href;
        
                // Check if the current URL contains the link's URL
                if (currentURL.indexOf(linkURL) !== -1) {
                    // Add the "active" class to the link
                    navLinks[i].classList.add('active');
                }
            }
        </script>
    </div>
    

    <div class="n-title">
        <h1>
            Z-GMOT with MA-SORT: Zero-shot Generic Multiple Object Tracking (GMOT) with Motion Appearance SORT (MA-SORT)
        </h1>
    </div>
    <div class="n-byline">
        <div class="byline">
            <ul class="authors">
                <li>
                    This work has been submitted to NAACL 2024
                </li>
            </ul>
            <p style="text-align: center; color: slategray">PDF version will be published soon</p>
            <!-- <ul class="authors affiliations">
                <li>
                    Anonymous
                </li>
            </ul> -->
            <ul class="authors links">
                <!-- <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fa fa-file-pdf"></i> Paper</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Code</button>
                    </a>
                </li>
                <li>
                    <a href="" target="_blank">
                        <button disabled class="btn"><i class="fab fa-github"></i> Dataset</button>
                    </a>
                </li>  -->
                <!-- <li>
                    <a href="https://www.youtube.com/playlist?list=PLSf1X3oNUW2se2U111HJ3wkSnZ7dcVGC_" target="_blank">
                        <button class="btn"><i class="fab fa-youtube fa-w-18"></i> Demo</button>
                    </a>
                </li> -->
            </ul>
        </div>
    </div>

<!-- <iframe width="760" height="381" src="https://www.youtube.com/embed/IvxeJRg4rYg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    
    <div class="n-article">

        <h2 id="introduction">
            Introduction
        </h2>
        <p>
            Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories, and struggles with unseen objects. To address this, Generic Multiple Object Tracking (GMOT) requires less prior information, but existing GMOT methods rely on initial bounding boxes and struggle with variations e.g., viewpoint, lighting, occlusion, scale, etc. We propose <b>Z-GMOT</b>, capable of tracking never-seen categories with no training examples, without predefined categories or initial bounding boxes. Our approach includes <b>iGLIP</b>, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. Addressing challenges in tracking high-similarity objects within the same category, we introduce <b>MA-SORT</b>, which integrates motion and appearance-based matching. Extensive evaluation on GMOT-40 dataset, AnimalTrack test set, DanceTrack validation set demonstrates substantial improvements. Our code and models will be publicly available upon paper acceptance.
        </p>
        <div>
            <img class="figure" src="media/ZGMOT.jpg" width="100%" alt="Pipeline Overview">
        </div>


        <!-- <h2 id="rebuttal-aaai">
            Rebuttal Section
        </h2>
        <div class="chat-container">
            <div class="recipient-message">
                <strong>Reviewer #1:</strong> Hello, how are you today?
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> I'm doing well, thank you! How can I help you?
            </div>
            <br>
    
            <div class="recipient-message">
                <strong>Reviewer #2:</strong> I have a question about your product.
            </div>
    
            <div class="sender-message">
                <strong>Answer:</strong> Sure, I'd be happy to help. What's your question?
            </div>
            <br>
        </div>
        <p> </p> -->
        
        <!------------------------------------------------------Section Boundary-------------------------------------------------->
        <h2 id="method">
            Limitations
        </h2>
        <p style="font-size: 22px;">
            We empirically observe several limitations of GLIP as follows:
        </p>
        <p>
            <b><u><span style="font-size: 20px;">Limitation 1 - High False Positives:</span></u></b> 
            GLIP performs well in recognizing general object categories (e.g., 'car'), but encounters challenges when dealing with 
            specific object categories that involve detailed property descriptions like color or shape (e.g., 'red car'). Figure 1 
            illustrates that when different thresholds are applied, GLIP's detection model consistently identifies general object 
            categories, maintaining stable results. However, when it comes to specific object categories, the detected bounding boxes 
            show considerable variability. At high thresholds, true positives (correctly identified objects) are frequently missed, 
            while at low thresholds, false positives (incorrectly identified objects) are prevalent. Balancing high true positives and 
            low false positives becomes a challenging trade-off, making it difficult to achieve optimal performance in specific object 
            category recognition.
        </p>
        <div>
            <img class="figure" src="media/limit1.png" width="100%" alt="Limitation 1">
        </div>

        <p>
            <b><u><span style="font-size: 20px;">Limitation 2 - Deep fusion in GLIP weakens proposal features when dealing with specific object category:</span></u></b> 
            In GLIP, the way text is processed and represented (text embedding) plays a crucial role, affecting both individual 
            elements (tokens) and overall features of proposed objects. The specific characteristics of tokens (${P^i}$) and proposed 
            objects (${O^i}$) are combined using a deep fusion module, allowing text embedding to continually influence the features of 
            proposed objects throughout this process. In Figure 2, we illustrate how text embedding impacts GLIP's performance at the 
            same threshold (<i>t</i>) for two scenarios: general object recognition (e.g., 'car' represented as 
            <sup>OC<sup>Gen</sup></sup>) and specific object recognition (e.g., 'red car' represented as <sup>OC<sup>Spe</sup></sup>). 
            While GLIP consistently performs well for general object recognition, the same cannot be said for specific object 
            recognition, even when using the same threshold. Our experiments indicate that the impact of text embedding varies—it can 
            either enhance (for general object recognition) or weaken (for specific object recognition) the features of proposed objects, 
            despite the presence of strong visual features.
        </p>
        <div>
            <img class="figure" src="media/limit2.png" width="100%" alt="Limitation 2">
        </div>

        <h2 id="method">
            Methodology
        </h2>
        <p style="font-size: 22px;">
            <b>iGLIP</b>: In order to overcome the limitations of GLIP as discussed earlier, we propose iGLIP.
        </p>
        <p>To overcome a specific limitation (referred to as Limitation 1), we suggest a solution where we use a <strong>high threshold <code>𝒯<sup>h</sup></code></strong> for recognizing specific objects (<sup>OC<sup>Spe</sup></sup>) and a <strong>low threshold <code>𝒯<sup>l</sup></code></strong> for general objects (<sup>OC<sup>Gen</sup></sup>). This approach ensures that only true positives (correctly identified objects) are detected for specific objects, while allowing for the acceptance of false positives in the case of general objects. Proposals from specific objects are treated as queries, and we introduce a mechanism called <strong>Query-Guided Matching</strong> to filter out all false positive proposals from general objects.</p>

        <p>Additionally, to address another limitation (Limitation 2), we propose using rich visual features obtained from the initial visual encoder layer (<code>O<sup>0</sup></code>) instead of the last layer feature (<code>O<sup>L</sup></code>) in our Query-Guided Matching process. The overall architecture of our proposed <em>iGLIP</em> is illustrated in Figure 2 (B).</p>

        <p><em>iGLIP</em> takes an input image (<code>I</code>) and two types of prompts: a specific prompt (<code>𝑇<sub>s</sub></code>) and a general prompt (<code>𝑇<sub>g</sub></code>). Both prompts go through a text encoder, i.e., <code>BERTModule</code>, to obtain contextual word features (i.e., <code>P<sub>s</sub><sup>0</sup></code> and <code>P<sub>g</sub><sup>0</sup></code>). Simultaneously, the input image goes through a visual encoder, i.e., <code>DyHeadModule</code>, to acquire proposal features (i.e., <code>O<sup>0</sup></code>). Then, <code>L</code> deep fusion layers are applied into <code>P<sub>s</sub><sup>0</sup></code>, <code>P<sub>g</sub><sup>0</sup></code>, and <code>O<sup>0</sup></code>. Finally, the word-region alignment module is utilized to calculate the alignment score by performing a dot product between the deep fused features.</p>

        <p>The resulting bounding boxes are filtered using different thresholds. A <span class="high-threshold">high threshold <code>𝒯<sup>h</sup></code></span> is applied to <b><code>general prompt's result (general_result)</code></b> to ensure only true positives are detected, and a <span class="low-threshold">low threshold <code>𝒯<sup>l</sup></code></span> is applied to <b><code>specific prompt's result (specific_result)</code></b> to include all object proposals, which may include false positives. All true positives obtained by <b><code>specific_result</code></b> are treated as a query set <span class="variable">𝒬<sub>q</sub></span> (template patterns), while all bounding boxes obtained by <b><code>general_result</code></b> form the target set <span class="variable">𝒬<sub>t</sub></span>. A <span class="method">Query-Guided Matching</span> module is then proposed to eliminate false positives in <span class="variable">𝒬<sub>t</sub></span> by using <span class="variable">𝒬<sub>q</sub></span> as template patterns.</p>

        <div>
            <img class="figure" src="media/glip-iglip.png" width="100%" alt="Limitation 1">
        </div>

        <p>
            <b><u><span style="font-size: 20px;">Query-Guided Matching:</span></u></b>
        </p>
        
        <p>To address the limitation of GLIP's deep fusion module not paying attention to specific properties, we propose a <strong style="font-family: 'Times New Roman', Times, serif;">Query-Guided Matching</strong> mechanism to filter false positives from the general object category set <span style="font-family: 'Times New Roman', Times, serif;">𝓑<sub>t</sub></span>. Specifically, we use the TP obtained by the specific prompt as template patterns <span style="font-family: 'Times New Roman', Times, serif;">𝓑<sub>q</sub></span> and query back on <span style="font-family: 'Times New Roman', Times, serif;">𝓑<sub>t</sub></span> to find objects with similar visual properties. To perform the matching, we propose to utilize only visual features <span style="font-family: 'Times New Roman', Times, serif;">𝑂<sup>0</sup></span> extracted from the backbone (without the influence of text embeddings).</p>

        <p>Let <span style="font-family: 'Times New Roman', Times, serif;">𝑂<sup>0</sup><sub>t</sub></span> and <span style="font-family: 'Times New Roman', Times, serif;">𝑂<sup>0</sup><sub>q</sub></span> represent the visual features of object proposals in <span style="font-family: 'Times New Roman', Times, serif;">𝓑<sub>t</sub></span> and <span style="font-family: 'Times New Roman', Times, serif;">𝓑<sub>q</sub></span>, the matching score is defined as the cosine similarity:</p>

        <blockquote>
            <p><span style="font-family: 'Times New Roman', Times, serif;">S<sub>qt</sub> = cos(𝑂<sup>0</sup><sub>q</sub> &middot; (𝑂<sup>0</sup><sub>t</sub>)<sup>T</sup>)</span></p>
        </blockquote>

        <p>The final detection results comprise the query objects and candidate objects with high similarity.</p>



        <p style="font-size: 22px;">
            <b>MA-SORT</b>: We introduce MA-SORT, as an enhancement of OC-SORT, which itself extends the SORT
        </p>
        <p>
            The standard similarity between track and box embeddings is defined using cosine distance, denoted as <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝐂<sub>𝑎</sub> ∈ ℜ<sup>𝑀×𝑁</sup></span>. In a typical tracking approach that combines visual appearance and motion cues, the cost matrix <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝐶</span> is computed as <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝐶(𝒯, 𝒟) = 𝑀<sub>c</sub>(𝒯, 𝒟) + 𝛼 𝐶<sub>𝑎</sub>(𝒯, 𝒟)</span>, where <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝑀<sub>c</sub></span> represents the motion cost, measured by the IoU cost matrix. Leveraging OC-SORT, which computes a virtual trajectory over the occlusion period to rectify the error accumulation of filter parameters during occlusions, the motion cost is defined as <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝑀<sub>c</sub>(𝒯, 𝒟) = IoU(𝒯, 𝒟) + 𝜆 𝐶<sub>𝑣</sub>(𝜏, 𝒯)</span>. Thus, the resulting cost matrix integrating both visual appearance and motion is as follows:
        </p>
          
        <figure>
          <p style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold; text-align: center;">
            𝐶(𝒯, 𝒟) = IoU(𝒯, 𝒟) + 𝜆 C<sub>𝑣</sub>(𝜏, 𝒯) + 𝛼 C<sub>𝑎</sub>(𝒯, 𝒟)
          </p>
          <figcaption style="text-align: center;">(Equation 1)</figcaption>
        </figure>
        <p>
          As the weight on appearance decreases, we propose redistributing the remaining weight to motion. Thus, the adaptive motion weight 
          <span style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold;">𝑊<sub>amw</sub></span> is defined as:
        </p>
        
        <figure>
          <p style="font-family: 'Times New Roman', serif; font-size: 20px; font-weight: bold; text-align: center;">
            𝑊<sub>amw</sub> = 1 + [1 - 𝑊<sub>aaw</sub>] = 2 - [1 - 𝜇<sub>det</sub> / (1 - cos(45°))]
          </p>
          <figcaption style="text-align: center;">(Equation 2)</figcaption>
        </figure>
    </div>

    <footer>
        <div class="footer-content">
            <p style="text-align: center;">&copy; Website for NAACL24's submission</p>
        </div>
    </footer>
</body>

</html>
    
                   
